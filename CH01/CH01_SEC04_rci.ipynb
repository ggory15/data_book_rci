{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Pseudo-Inverse, Least-Squares, and Regression\n",
    "* Usage of SVD\n",
    "    * Linear systems of eqiations $Ax = b$에 대해서 해가 없는 over determined의 경우, $|Ax-b|_2^2$을 최소화 하는 값을 찾아야 된다.\n",
    "    * 이 방법을 least-squares solution이라 한다.\n",
    "    * 해가 무한한 경우, $||x||_2$을 최소화 하는 값을 찾아야 한다.\n",
    "    * 이 방법을 minimum-norm solution이라 한다.\n",
    "    * SVD는 이 문제를 최적화 하는 방법이다.\n",
    "    * Full SVD가 아닌 $\\Sigma$의 비대각 부분과 대각 원소 중 singular value가 0인 부분을 모두 제거하고 제거된 $\\Sigma$에 대응되는 $U$와 $V$의 원소도 함께 제거해 차원을 줄인 Truncated SVD를 적용한다.\n",
    "    * $A = \\~{U}\\~{\\Sigma}\\~{V^*}$의 A에 $\\~{a}, \\~{\\Sigma}, \\~{V^*}$ 역을 취할 수 있다. \n",
    "    * $left$ $pseudo-inverse$의 결과를 얻을 수 있다.\n",
    "    * $A^+ = \\~{V}\\~{\\Sigma}^{-1}\\~{U^*}$ -> $A^+A = \\~{V}\\~{V}^*$\n",
    "    * truncated SVD가 0이 아닌 모든 singular value를 잡아야 $A^+A=I_{m \\times m}$가 성립한다.\n",
    "    * 위를 이용해 $x$값을 구하고 $Ax = b$에 대입하면 $A\\~{x} = \\~{U}\\~{U^*}b$를 얻을 수 있다.\n",
    "    * b가 \\~{U}의 열에 존재할 때만 \\~{x}가 성립한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Condition Number\n",
    "    * A행렬의 condition number란 행렬이 얼마나 민감한지 나타내는 척도인데 이는 행렬의 특이값과 연관이 있다.\n",
    "    * $k(A) = {\\sigma}_{max}(A)/{\\sigma}_{min}(A) $\n",
    "    * condition number을 줄이기 위해서는 SVD를 더 transcate를 하는 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-Dimensional Linear Regression\n",
    "    * 회귀는 데이터를 기반으로 변수를 연관시키는 중요한 통계적 도구이다.\n",
    "    * 독립변수 x가 1개일 때의 회귀이다.\n",
    "    * 데이터가 선형적 관계에 있다 가정할 때, $[b] = [a]x = \\~{U}\\~{\\Sigma}\\~{V}^*x$ -> $x = \\~{V}\\~{\\Sigma}^{-1}\\~{U^*}b$이다.\n",
    "    * least-square해를 찾기 위해 pseudo-inverse를 취하면 $x = a*b/||a||^2_2$ 를 얻을 수 있다.\n",
    "    * x가 벡터a를 벡터b로 최적의 매핑을 한다 생각하면 x는 b의 dot product를 a 방향으로 normalize하며 구할 수 있다.\n",
    "    * 두번째 normalizing factor $||a||_2$를 더해 구해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "x = 3 # True slope\n",
    "a = np.arange(-2,2,0.25)\n",
    "a = a.reshape(-1, 1)\n",
    "b = x*a + np.random.randn(*a.shape) # Add noise\n",
    "# y = ax + b\n",
    "plt.plot(a, x*a, color='k', linewidth=2, label='True line') # True relationship\n",
    "plt.plot(a, b, 'x', color='r', MarkerSize = 10, label='Noisy data') # Noisy measurements\n",
    "\n",
    "U, S, VT = np.linalg.svd(a,full_matrices=False)\n",
    "xtilde = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ b # Least-square fit\n",
    "\n",
    "plt.plot(a,xtilde * a,'--',color='b',linewidth=4, label='Regression line')\n",
    "\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three methods of computing regression\n",
    "\n",
    "xtilde1 = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ b\n",
    "xtilde2 = np.linalg.pinv(a) @ b\n",
    "\n",
    "# The third method is specific to Matlab:\n",
    "# xtilde3 = regress(b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-linear Regression\n",
    "    * 선형 회귀와 동일하지만 2차원에서 n차원으로 늘리는 차이가 있다.\n",
    "    * 목표는 x를 weighting해줘서 어떠한 요소가 가장 영향을 주는지 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Load dataset\n",
    "A = np.loadtxt(os.path.join('..','DATA','hald_ingredients.csv'),delimiter=',')\n",
    "b = np.loadtxt(os.path.join('..','DATA','hald_heat.csv'),delimiter=',')\n",
    "# y = Ax - b\n",
    "\n",
    "# Solve Ax=b using SVD\n",
    "U, S, VT = np.linalg.svd(A,full_matrices=0)\n",
    "x = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ b\n",
    "\n",
    "plt.plot(b, color='k', linewidth=2, label='Heat Data') # True relationship\n",
    "plt.plot(A@x, '-o', color='r', linewidth=1.5, markersize=6, label='Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보스턴 집값을 예측하는 예시인데 집값과 상관있는 13가지 요소를 회귀분석하여 추세와 가중 요소를 판단할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Load dataset\n",
    "H = np.loadtxt(os.path.join('..','DATA','housing.data'))\n",
    "b = H[:,-1] # housing values in $1000s\n",
    "A = H[:,:-1] # other factors\n",
    "\n",
    "# Pad with ones for nonzero offset\n",
    "A = np.pad(A,[(0,0),(0,1)],mode='constant',constant_values=1)\n",
    "\n",
    "\n",
    "# Solve Ax=b using SVD\n",
    "# Note that the book uses the Matlab-specific \"regress\" command\n",
    "U, S, VT = np.linalg.svd(A,full_matrices=0)\n",
    "x = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ b\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "\n",
    "plt.plot(b, color='k', linewidth=2, label='Housing Value') # True relationship\n",
    "plt.plot(A@x, '-o', color='r', linewidth=1.5, markersize=6, label='Regression')\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.ylabel('Median Home Value [$1k]')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "sort_ind = np.argsort(H[:,-1])\n",
    "plt.plot(b[sort_ind], color='k', linewidth=2, label='Housing Value') # True relationship\n",
    "plt.plot(A[sort_ind,:]@x, '-o', color='r', linewidth=1.5, markersize=6, label='Regression')\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = np.mean(A,axis=0)\n",
    "A_mean = A_mean.reshape(-1, 1)\n",
    "\n",
    "A2 = A - np.ones((A.shape[0],1)) @ A_mean.T\n",
    "\n",
    "for j in range(A.shape[1]-1):\n",
    "    A2std = np.std(A2[:,j])\n",
    "    A2[:,j] = A2[:,j]/A2std\n",
    "    \n",
    "A2[:,-1] = np.ones(A.shape[0])\n",
    "\n",
    "U, S, VT = np.linalg.svd(A2,full_matrices=0)\n",
    "x = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ b\n",
    "x_tick = range(len(x)-1)+np.ones(len(x)-1)\n",
    "plt.bar(x_tick,x[:-1])\n",
    "plt.xlabel('Attribute')\n",
    "plt.ylabel('Significance')\n",
    "plt.xticks(x_tick)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
